# TP2 & TP5 - Visualisation et R√©duction de Dimension sur MNIST et jumeaux hybrides

## üìã Table des mati√®res
- [Description](#description)
- [TP2 : R√©duction de Dimension](#tp2--r√©duction-de-dimension)
- [TP2 - P2 : Auto-encodeurs](#tp5--auto-encodeurs)
- [Installation](#installation)
- [Structure du Projet](#structure-du-projet)
- [Utilisation](#utilisation)
- [M√©thodes Impl√©ment√©es](#m√©thodes-impl√©ment√©es)
- [R√©sultats](#r√©sultats)

## üìù Description

Ce projet explore diff√©rentes techniques de r√©duction de dimension et de visualisation appliqu√©es √† la base de donn√©es de chiffres manuscrits (digits dataset de scikit-learn). Il combine des m√©thodes classiques de r√©duction de dimension (TP2) avec des approches d'apprentissage profond utilisant des auto-encodeurs (TP5).

**Dataset utilis√© :** Digits (sklearn)
- 1797 √©chantillons
- Images 8x8 pixels (64 features)
- 10 classes (chiffres 0-9)

## üéØ TP2 : R√©duction de Dimension

### Objectifs
- Explorer et visualiser des donn√©es de haute dimension
- Comparer diff√©rentes m√©thodes de r√©duction de dimension
- Analyser la s√©parabilit√© des classes dans l'espace r√©duit

### M√©thodes Impl√©ment√©es

#### 1. **PCA (Principal Component Analysis)**
- M√©thode lin√©aire bas√©e sur la variance
- Rapide et d√©terministe
- Pr√©serve la variance globale des donn√©es
- Analyse de la variance expliqu√©e cumulative

#### 2. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**
- M√©thode non-lin√©aire
- Excellente pour la visualisation
- Pr√©serve les structures locales
- Tests avec diff√©rentes valeurs de perplexit√© (5, 30, 50)
- Optimisation de l'exag√©ration initiale

#### 3. **MDS (Multidimensional Scaling)**
- Pr√©serve les distances entre points
- Maintient la structure g√©om√©trique
- Comparaison m√©trique vs non-m√©trique
- Analyse du stress (qualit√© de la repr√©sentation)

#### 4. **UMAP (Uniform Manifold Approximation and Projection)**
- √âquilibre entre structure locale et globale
- Plus rapide que t-SNE
- Bonne pr√©servation de la topologie

## ü§ñ TP5 : Auto-encodeurs

### Objectifs
- Apprendre des repr√©sentations compress√©es par apprentissage profond
- Reconstruire les donn√©es √† partir de l'espace latent
- Comparer avec les m√©thodes classiques

### Architecture Impl√©ment√©e

#### Auto-encodeur 2D
```
Encodeur: 64 ‚Üí 32 (ReLU) ‚Üí 16 (ReLU) ‚Üí 2 (Linear)
D√©codeur: 2 ‚Üí 16 (ReLU) ‚Üí 32 (ReLU) ‚Üí 64 (Sigmoid)
```

#### Auto-encodeur 3D
```
Encodeur: 64 ‚Üí 32 (ReLU) ‚Üí 16 (ReLU) ‚Üí 3 (Linear)
D√©codeur: 3 ‚Üí 16 (ReLU) ‚Üí 32 (ReLU) ‚Üí 64 (Sigmoid)
```

### Caract√©ristiques
- Fonction de perte : MSE (Mean Squared Error)
- Optimiseur : Adam
- 100 √©poques d'entra√Ænement
- Batch size : 32
- Validation split : 20%

### Visualisations
- √âvolution de la perte pendant l'entra√Ænement
- Espace latent 2D et 3D
- Reconstruction des images originales
- Comparaison qualitative des reconstructions

## üõ†Ô∏è Installation

### Pr√©requis
```bash
Python 3.8+
```

### D√©pendances
```bash
pip install numpy
pip install matplotlib
pip install pandas
pip install seaborn
pip install scikit-learn
pip install tensorflow  # Pour les auto-encodeurs
pip install umap-learn  # Optionnel pour UMAP
```

### Installation rapide
```bash
pip install -r requirements.txt
```

## üìÅ Structure du Projet

```
TP 2 MNIST/
‚îú‚îÄ‚îÄ TP2_MNIST.ipynb          # Notebook principal
‚îú‚îÄ‚îÄ Readme.md                # Ce fichier
‚îî‚îÄ‚îÄ requirements.txt         # D√©pendances Python
```

## üöÄ Utilisation

### Lancer le notebook
1. Ouvrir Jupyter Notebook ou VS Code
2. Charger `TP2_MNIST.ipynb`
3. Ex√©cuter les cellules s√©quentiellement

### Sections du notebook
1. **Chargement des donn√©es** - Import et exploration du dataset
2. **Visualisation initiale** - Affichage d'exemples de chiffres
3. **PCA** - R√©duction lin√©aire et analyse de variance
4. **t-SNE** - Exploration avec diff√©rents param√®tres
5. **MDS** - Pr√©servation des distances
6. **UMAP** - M√©thode moderne (optionnel)
7. **Auto-encodeurs** - Apprentissage profond
8. **Comparaison** - Vue d'ensemble de toutes les m√©thodes

## üìä M√©thodes Impl√©ment√©es

| M√©thode | Type | Avantages | Inconv√©nients |
|---------|------|-----------|---------------|
| **PCA** | Lin√©aire | Rapide, d√©terministe, interpr√©table | Ne capture pas les relations non-lin√©aires |
| **t-SNE** | Non-lin√©aire | Excellente s√©paration visuelle | Lent, non-d√©terministe, perd structure globale |
| **MDS** | Distance | Pr√©serve distances, structure g√©om√©trique | Co√ªteux en calcul, sensible au bruit |
| **UMAP** | Non-lin√©aire | Rapide, √©quilibre local/global | N√©cessite installation suppl√©mentaire |
| **Auto-encodeur** | Deep Learning | Reconstruction, flexible, non-lin√©aire | N√©cessite entra√Ænement, hyperparam√®tres |

## üìà R√©sultats

### Variance Expliqu√©e (PCA)
- 2 composantes : ~25% de variance
- 95% variance n√©cessite ~21 composantes

### S√©paration des Classes
- **t-SNE** : Meilleure s√©paration visuelle des clusters
- **MDS** : Bonne pr√©servation de la structure g√©om√©trique
- **Auto-encodeur** : S√©paration comparable, avec capacit√© de reconstruction

### Reconstruction (Auto-encodeur)
- MSE finale : ~0.01-0.02
- Visualisation fid√®le des chiffres apr√®s reconstruction

## üîç Analyses Compl√©mentaires

### Optimisation t-SNE
- Perplexit√© optimale : 30-40
- Early exaggeration : 12-20
- Impact significatif sur la qualit√© visuelle

### MDS M√©trique vs Non-m√©trique
- M√©trique : Pr√©serve distances exactes
- Non-m√©trique : Plus flexible, pr√©serve l'ordre

### Auto-encodeur 2D vs 3D
- 2D : Visualisation directe
- 3D : Meilleure capacit√© de repr√©sentation

## üìö R√©f√©rences

- **PCA**: Pearson, K. (1901). "On Lines and Planes of Closest Fit to Systems of Points in Space"
- **t-SNE**: van der Maaten & Hinton (2008). "Visualizing Data using t-SNE"
- **MDS**: Kruskal, J.B. (1964). "Multidimensional scaling by optimizing goodness of fit"
- **UMAP**: McInnes et al. (2018). "UMAP: Uniform Manifold Approximation and Projection"
- **Auto-encodeurs**: Hinton & Salakhutdinov (2006). "Reducing the Dimensionality of Data with Neural Networks"

## üë• Auteur

Projet r√©alis√© dans le cadre des TPs de visualisation de donn√©es et apprentissage automatique.

## üìÑ Licence

Ce projet est √† usage √©ducatif.

---

**Note**: Pour de meilleures performances, il est recommand√© d'ex√©cuter le notebook sur une machine avec au moins 8GB de RAM. Les auto-encodeurs b√©n√©ficient d'un GPU mais peuvent fonctionner sur CPU.
